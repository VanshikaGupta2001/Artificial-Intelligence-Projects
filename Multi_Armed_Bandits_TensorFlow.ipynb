{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**<center>Multi Armed Bandits**  \n",
        "  \n",
        "**AIM:**  \n",
        "To understand Multi Armed Bandits in TensorFlow.  \n",
        "  \n",
        "    \n",
        "**Exercise 1:**  \n",
        "To create an environment:-  \n",
        "1. For which the observation is a random integer between -5 and 5, there are 3 possible actions (0, 1, 2), and the reward is the product of the action and the observation.  \n",
        "2. Define an optimal policy manually. The action only depends on the sign of the observation, 0 when is negative and 2 when is positive.  \n",
        "3. Request for 50 observations from the environment, compute and print the total reward.  \n",
        "  \n",
        "    \n",
        "**Exercise 2:**  \n",
        "To create an environment:-   \n",
        "1. Define an environment will either always give   \n",
        "<center>reward = observation * action   \n",
        "<center> or  \n",
        " <center> reward = -observation * action.    \n",
        "\n",
        "This will be decided when the environment is initialized.  \n",
        "2. Define a policy that detects the behavior of the underlying environment. There are three situations that the policy needs to handle -  \n",
        "i. The agent has not detected know yet which version of the environment is running.  \n",
        "ii. The agent detected that the original version of the environment is running.  \n",
        "iii. The agent detected that the flipped version of the environment is running.  \n",
        "3. Define the agent that detects the sign of the environment and sets the policy appropriately.  \n",
        "  "
      ],
      "metadata": {
        "id": "t74NeokvjtOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**INITIALIZATION:**"
      ],
      "metadata": {
        "id": "69qjpRuvJ41T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary libraries\n",
        "\n",
        "#Libraries for data preprocessing and computations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Libraries for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import abc\n",
        "\n",
        "%matplotlib inline"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "id": "jEq2Jo-oQbl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing TensorFlow agents package\n",
        "!pip install tf-agents"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting tf-agents\n  Downloading tf_agents-0.15.0-py3-none-any.whl (1.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pygame==2.1.0\n  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (2.2.1)\nRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\nCollecting gym<=0.23.0,>=0.17.0\n  Downloading gym-0.23.0.tar.gz (624 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 KB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (4.5.0)\nRequirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from tf-agents) (8.4.0)\nRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.4.0)\nRequirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.5.0)\nRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\nRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.22.4)\nRequirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (3.19.6)\nRequirement already satisfied: tensorflow-probability>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.19.0)\nRequirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\nRequirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (6.0.0)\nRequirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.4.0)\nRequirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (4.4.2)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.1.8)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.15.0)\nBuilding wheels for collected packages: gym\n  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697660 sha256=6ef6114e2f3e115c305c116cb4ce7fbe161f1be7498bc9c828e75ba12b54be69\n  Stored in directory: /root/.cache/pip/wheels/e7/2f/ab/68bf956c5dde73c1856d981e54292cf58385fb60bca10b7acd\nSuccessfully built gym\nInstalling collected packages: pygame, gym, tf-agents\n  Attempting uninstall: gym\n    Found existing installation: gym 0.25.2\n    Uninstalling gym-0.25.2:\n      Successfully uninstalled gym-0.25.2\nSuccessfully installed gym-0.23.0 pygame-2.1.0 tf-agents-0.15.0\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "id": "nAs2jW9TllzN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eab86e0e-152b-49bb-feab-1ee9f2afe9d0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import for environment\n",
        "\n",
        "#Importing TensorFlow library\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents import tf_agent #Imports agents\n",
        "from tf_agents.drivers import driver  #Imports driver\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.trajectories import policy_step\n",
        "\n",
        "nest = tf.nest"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "JbvFHuvZKpIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**OBJECTIVE 1:**"
      ],
      "metadata": {
        "id": "dlfzI0kSLWjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Environment**"
      ],
      "metadata": {
        "id": "93JLG0hYLVFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining a custom environment class named Bandit\n",
        "class Bandit(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self, obs_spec, act_spec): #here the constructor takes two arguments\n",
        "    self._obs_spec = obs_spec\n",
        "    self._act_spec = act_spec\n",
        "    super(Bandit, self).__init__()  #Constructor calls the constructor of the superclass\n",
        "\n",
        "  #Helper functions.\n",
        "\n",
        "  #Returns the action specs of the environment\n",
        "  def action_spec(self):\n",
        "    return self._act_spec #returns the action spec passed to the constructor.\n",
        "\n",
        "  #Returns the observation specs of the environment\n",
        "  def observation_spec(self):\n",
        "    return self._obs_spec #returns the observation spec passed to the constructor.\n",
        "\n",
        "  #Returns an empty observation of the same shape and dtype as the observation spec of the environment.\n",
        "  def _empty_obs(self):\n",
        "    return tf.nest.map_structure(lambda x: np.zeros(x.shape, x.dtype), self.obs_spec())\n",
        "\n",
        "  #The following 2 functions can't be overwridden by subclass\n",
        "  def _reset(self):\n",
        "    #Returns a time step of observation\n",
        "    return ts.restart(self._observe(), batch_size=self.batch_size)\n",
        "\n",
        "  def _step(self, action):\n",
        "    #Returns a time step of reward\n",
        "    reward = self._apply_action(action)\n",
        "    return ts.termination(self._observe(), reward)\n",
        "\n",
        "  #The following 2 functions are implemented in subclass\n",
        "  @abc.abstractmethod\n",
        "  def _observe(self):\n",
        "    #Returns observation.\n",
        "    self._observation = np.random.randint(-2, 3, (1,), dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _apply_action(self, action):\n",
        "    #Applies action to the Environment and returns the corresponding reward.\n",
        "    return action * self._observation"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "VnFI7MTBMaK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This code defines a new environment named \"Env_1\" that is a subclass of the previously defined \"Bandit\" environment.\n",
        "class Env_1(Bandit):\n",
        "\n",
        "  def __init__(self):\n",
        "    act_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    obs_spec = array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=-5, maximum=5, name='observation')\n",
        "    super(Env_1, self).__init__(obs_spec, act_spec)\n",
        "\n",
        "  def _observe(self):\n",
        "    self._observation = np.random.randint(-2, 3, (1,), dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  def _apply_action(self, action):\n",
        "    return action * self._observation #This method multiplies the input action by the current observation value and returns the resulting reward."
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "id": "jYXisd5pK1SJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Policy**"
      ],
      "metadata": {
        "id": "VoCSPK0zOCwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defines a new policy named \"Pol_1\" that is a subclass of the \"tf_policy.TFPolicy\" class.\n",
        "class Pol_1(tf_policy.TFPolicy):\n",
        "  def __init__(self):\n",
        "    obs_spec = tensor_spec.BoundedTensorSpec(shape=(1,), dtype=tf.int32, minimum=-5, maximum=5)\n",
        "    ts_spec = ts.time_step_spec(obs_spec)\n",
        "\n",
        "    act_spec = tensor_spec.BoundedTensorSpec(shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
        "\n",
        "    super(Pol_1, self).__init__(time_step_spec=ts_spec, action_spec=act_spec)\n",
        "  def _distribution(self, time_step):\n",
        "    pass\n",
        "\n",
        "  def _variables(self):\n",
        "    return ()\n",
        "\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    obs_sign = tf.cast(tf.sign(time_step.observation[0]), dtype=tf.int32)\n",
        "    action = obs_sign + 1\n",
        "    return policy_step.PolicyStep(action, policy_state)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "id": "SwW-_JGLN3vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Implement**"
      ],
      "metadata": {
        "id": "gdl7hsX2PfM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize class\n",
        "env1 = Env_1()\n",
        "tf_env1 = tf_py_environment.TFPyEnvironment(env1)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "id": "FG6ThPhjQwc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#50 Observations\n",
        "policy = Pol_1()\n",
        "step = tf_env1.reset()\n",
        "obs = 50\n",
        "trans = []\n",
        "reward = 0\n",
        "for i in range(obs):\n",
        "  action = policy.action(step).action\n",
        "  next_step = tf_env1.step(action)\n",
        "  trans.append([step, action, next_step])\n",
        "  reward += next_step.reward\n",
        "  cur_obs = step.observation\n",
        "  print('Action: \\n', action)\n",
        "  print(\"Reward: \\n\", reward)\n",
        "  print(\"Oservation: \\n\", cur_obs)\n",
        "  step = next_step\n",
        "\n",
        "np_trans = tf.nest.map_structure(lambda x: x.numpy(), trans)\n",
        "print('Total reward: \\n', reward.numpy())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Action: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[1]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[6.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([1], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[6.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[0]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[6.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[10.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[12.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[1]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[12.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[16.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[20.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[20.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[22.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[1]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[24.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[1]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[24.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([1], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[24.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[0]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([1], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[24.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[0]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[26.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[1]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([1], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[26.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[0]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[26.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[26.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([1], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[26.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[0]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[26.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[26.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[26.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[30.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[30.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[30.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[34.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[34.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[38.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[42.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[44.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[1]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([1], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[44.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[0]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([1], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[44.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[0]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([1], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[44.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[0]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[48.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[50.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[1]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([1], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[50.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[0]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[54.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([1], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[54.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[0]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[54.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[54.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([1], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[54.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[0]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[54.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[58.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[58.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([2], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[62.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[2]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([1], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[62.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[0]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[62.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-1]], shape=(1, 1), dtype=int32)\nAction: \n tf.Tensor([0], shape=(1,), dtype=int32)\nReward: \n tf.Tensor([[62.]], shape=(1, 1), dtype=float32)\nOservation: \n tf.Tensor([[-2]], shape=(1, 1), dtype=int32)\nTotal reward: \n [[62.]]\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hORE4bZPRUW",
        "outputId": "2864f645-12c9-4dd2-c70f-be4f4e2c9aca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inferences: \n",
        "\n",
        "1. The Bandit Environemt and Tensorflow Environment have been created.  \n",
        "2. The Sign Policy has been implemented.  \n",
        "3. The actions and reward over 50 observations have been found.  \n",
        "4. The total reward is calculated.   \n",
        "\n"
      ],
      "metadata": {
        "id": "HVP1vlXNV9pV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**OBJECTIVE 2:**"
      ],
      "metadata": {
        "id": "yr4-kyxbXa_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Environment**"
      ],
      "metadata": {
        "id": "giDYmi9DXxYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Environment\n",
        "\n",
        "class Env_2(Bandit):\n",
        "\n",
        "  def __init__(self):\n",
        "    act_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    obs_spec = array_spec.BoundedArraySpec(shape=(1,), dtype=np.int32, minimum=-5, maximum=5, name='observation')\n",
        "\n",
        "    #Flip the sign with probability 0.5.\n",
        "    self._reward_sign = 2 * np.random.randint(2) - 1\n",
        "    print(\"reward sign:\")\n",
        "    print(self._reward_sign)\n",
        "\n",
        "    super(Env_2, self).__init__(obs_spec, act_spec)\n",
        "\n",
        "  def _observe(self):\n",
        "    self._observation = np.random.randint(-2, 3, (1,), dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  def _apply_action(self, action):\n",
        "    return self._reward_sign * action * self._observation[0]\n"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "id": "EayVY1eOQTY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Policy**\n"
      ],
      "metadata": {
        "id": "zo6puHgEYV7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Policy\n",
        "\n",
        "class Pol_2(tf_policy.TFPolicy):\n",
        "  def __init__(self, situation):\n",
        "    obs_spec = tensor_spec.BoundedTensorSpec(shape=(1,), dtype=tf.int32, minimum=-5, maximum=5)\n",
        "    act_spec = tensor_spec.BoundedTensorSpec(shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
        "    ts_spec = ts.time_step_spec(obs_spec)\n",
        "    self._situation = situation\n",
        "    super(Pol_2, self).__init__(time_step_spec=ts_spec, action_spec=act_spec)\n",
        "  def _distribution(self, time_step):\n",
        "    pass\n",
        "\n",
        "  def _variables(self):\n",
        "    return [self._situation]\n",
        "\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    sign = tf.cast(tf.sign(time_step.observation[0, 0]), dtype=tf.int32)\n",
        "    def case_unknown_fn():\n",
        "      #Choose 1 for information about sign\n",
        "      return tf.constant(1, shape=(1,))\n",
        "\n",
        "    #Choose 0 or 2, based on the situation and the sign of the observation.\n",
        "    def case_normal_fn():\n",
        "      return tf.constant(sign + 1, shape=(1,))\n",
        "    def case_flipped_fn():\n",
        "      return tf.constant(1 - sign, shape=(1,))\n",
        "\n",
        "    cases = [(tf.equal(self._situation, 0), case_unknown_fn),\n",
        "             (tf.equal(self._situation, 1), case_normal_fn),\n",
        "             (tf.equal(self._situation, 2), case_flipped_fn)]\n",
        "    action = tf.case(cases, exclusive=True)\n",
        "    return policy_step.PolicyStep(action, policy_state)"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "id": "EEadCWxIYU9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Agent**"
      ],
      "metadata": {
        "id": "Ta4iVxXyaKJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Agent\n",
        "\n",
        "class Agent(tf_agent.TFAgent):\n",
        "  def __init__(self):\n",
        "    self._situation = tf.Variable(0, dtype=tf.int32)\n",
        "    policy = Pol_2(self._situation)\n",
        "    ts_spec = policy.time_step_spec\n",
        "    act_spec = policy.action_spec\n",
        "    super(Agent, self).__init__(time_step_spec=ts_spec, action_spec=act_spec,policy=policy,collect_policy=policy,train_sequence_length=None)\n",
        "\n",
        "  def _initialize(self):\n",
        "    return tf.compat.v1.variables_initializer(self.variables)\n",
        "\n",
        "  def _train(self, experience, weights=None):\n",
        "    observation = experience.observation\n",
        "    action = experience.action\n",
        "    reward = experience.reward\n",
        "    \n",
        "    #Change the value of the situation variable if it is unknown (0) right now, and infer the situation only if the observation is not 0.\n",
        "    needs_action = tf.logical_and(tf.equal(self._situation, 0), tf.not_equal(reward, 0))\n",
        "\n",
        "\n",
        "    def new_situation_fn():\n",
        "      #Returns either 1 or 2, depending on the sign.\n",
        "      return (3 - tf.sign(tf.cast(observation[0, 0, 0], dtype=tf.int32) *tf.cast(action[0, 0], dtype=tf.int32) *tf.cast(reward[0, 0], dtype=tf.int32))) / 2\n",
        "\n",
        "    new_situation = tf.cond(needs_action,new_situation_fn, lambda: self._situation)\n",
        "    new_situation = tf.cast(new_situation, tf.int32)\n",
        "    tf.compat.v1.assign(self._situation, new_situation)\n",
        "    return tf_agent.LossInfo((), ())"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "id": "ggEJjFR8ZgmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Trajectory\n",
        "\n",
        "#Add another dimension here because the agent expects the trajectory of shape [batch_size, time, ...], but both batch size and time are 1. Hence all the expand_dims.\n",
        "def trajec_bandit(initial_step, action_step, final_step):\n",
        "  return trajectory.Trajectory(observation=tf.expand_dims(initial_step.observation, 0), action=tf.expand_dims(action_step.action, 0), policy_info=action_step.info,reward=tf.expand_dims(final_step.reward, 0),discount=tf.expand_dims(final_step.discount, 0),step_type=tf.expand_dims(initial_step.step_type, 0),next_step_type=tf.expand_dims(final_step.step_type, 0))"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "id": "Ud_K4h8McRNu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Implement**"
      ],
      "metadata": {
        "id": "fN4VCg3kbVQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize class\n",
        "env2 = Env_2()\n",
        "tf_env2 = tf_py_environment.TFPyEnvironment(env2)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "reward sign:\n-1\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxcBJiP3a9QE",
        "outputId": "9efdd4b8-cd07-487c-f8fc-070b0562dc40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#50 Observations\n",
        "agent = Agent()\n",
        "step = tf_env2.reset()\n",
        "for i in range(50):\n",
        "  action_step = agent.collect_policy.action(step)\n",
        "  next_step = tf_env2.step(action_step.action)\n",
        "  experience = trajec_bandit(step, action_step, next_step)\n",
        "  print(\"Experience \", i, \" : \\n \", experience)\n",
        "  agent.train(experience)\n",
        "  step = next_step\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Experience  0  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>})\nExperience  1  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  2  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  3  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  4  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  5  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[0]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  6  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[0]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  7  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  8  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  9  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[0]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  10  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  11  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  12  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  13  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  14  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  15  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  16  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  17  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  18  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[0]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  19  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[0]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  20  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  21  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  22  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  23  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  24  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  25  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  26  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  27  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  28  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  29  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  30  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  31  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  32  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  33  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  34  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  35  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  36  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  37  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  38  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  39  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  40  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[0]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  41  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  42  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  43  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  44  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  45  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-2]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[4.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  46  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[0]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  47  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[0]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  48  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[-1]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\nExperience  49  : \n  Trajectory(\n{'action': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[1]], dtype=int32)>,\n 'discount': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'next_step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>,\n 'observation': <tf.Tensor: shape=(1, 1, 1), dtype=int32, numpy=array([[[0]]], dtype=int32)>,\n 'policy_info': (),\n 'reward': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.]], dtype=float32)>,\n 'step_type': <tf.Tensor: shape=(1, 1), dtype=int32, numpy=array([[2]], dtype=int32)>})\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb3BeFCAdDV3",
        "outputId": "454febcb-779d-45a1-b107-da5a9c123f3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inferences: \n",
        "1. The Tensorflow Environment has been created.  \n",
        "2. The Sign Policy and Sign Agent has been implemented.  \n",
        "3. The Trajectory/training of agent over 50 observations has been completed.  \n",
        "4. The reward is non-negative after the 2nd iteration/experience (unless the observation is 1, in the 1st iteration) as the policy chooses action correctly.    \n",
        "\n"
      ],
      "metadata": {
        "id": "XuAFi8wLfOn4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}