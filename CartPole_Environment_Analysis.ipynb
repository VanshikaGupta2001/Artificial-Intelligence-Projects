{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**CartPole - V0 Environment**  \n",
        "  \n",
        "**AIM:**  \n",
        "Build a simple agent.  \n",
        "  \n",
        "**OBJECTIVE:**  \n",
        "Use the CartPole-v0 environment and write a program to :-  \n",
        "1. Implement the CartPole environment for a certain number of steps  \n",
        "2. Implement the CartPole environment for a certain number of episodes  \n",
        "3. Compare and comment on the rewards earned for both approaches.  \n",
        "  "
      ],
      "metadata": {
        "id": "9KbdezeDzv-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-agents[reverb]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "_MTItUKDDoo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import abc\n",
        "import tensorflow as tf\n",
        "\n",
        "#Importing library for mathematical computation\n",
        "import numpy as np\n",
        "\n",
        "#Importing environment related modules\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "id": "S90We7erG4tA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement the CartPole environment for a certain number of steps\n",
        "env = suite_gym.load('CartPole-v0') #Load Environment\n",
        "\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "time_step = tf_env.reset()  # reset() creates the initial time_step after resetting the environment.\n",
        "#defining variables\n",
        "num_steps = 500 #Defining number of steps\n",
        "transitions = []  #Creating empty transitions list\n",
        "reward = 0  #Initializing reward to 0\n",
        "\n",
        "for i in range(num_steps):\n",
        "  action = tf.constant([i % 2])\n",
        "  next_time_step = tf_env.step(action)  # applies the action and returns the new TimeStep.\n",
        "  transitions.append([time_step, action, next_time_step])\n",
        "  reward = reward + next_time_step.reward #Calculating total reward\n",
        "  time_step = next_time_step\n",
        "\n",
        "np_transitions = tf.nest.map_structure(lambda x: x.numpy(), transitions)\n",
        "print('\\n'.join(map(str, np_transitions)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "9wG_U3XqHAOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying total reward\n",
        "print(\"Total reward over {} timesteps : {} \".format(num_steps,reward.numpy()))\n",
        "#Displaying average reward\n",
        "print(\"Average reward over {} timesteps : {} \".format(num_steps, reward.numpy()/num_steps))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Total reward over 500 timesteps : [488.] \nAverage reward over 500 timesteps : [0.976] \n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGuw2HQav0mp",
        "outputId": "f925ded3-cc21-47dd-8499-8d931ebf7f2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement the CartPole environment for a certain number of episodes\n",
        "env = suite_gym.load('CartPole-v0') #Load Environment\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "time_step = tf_env.reset()\n",
        "\n",
        "#defining variables\n",
        "rewards = []  #creating empty list for rewards \n",
        "steps = []  #creating empty list for steps\n",
        "num_episodes = 500  #definig number of steps to be 500\n",
        "\n",
        "for _ in range(num_episodes):\n",
        "  episode_reward = 0\n",
        "  episode_steps = 0\n",
        "  while not time_step.is_last():\n",
        "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
        "    time_step = tf_env.step(action) # applies the action and returns the new TimeStep.\n",
        "    episode_steps = episode_steps + 1\n",
        "    episode_reward += time_step.reward.numpy()  #total reward\n",
        "  rewards.append(episode_reward)\n",
        "  steps.append(episode_steps) #total number of steps\n",
        "  time_step = tf_env.reset()  # reset() creates the initial time_step after resetting the environment.\n",
        "\n",
        "num_steps = np.sum(steps)\n",
        "avg_length = np.mean(steps)\n",
        "avg_reward = np.mean(rewards)"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "id": "MWfsSJ-8HAwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying total reward\n",
        "print(\"Total reward over {} episodes : {} \".format(num_episodes, sum(rewards)))\n",
        "#Displaying average reward\n",
        "print('Average total reward over {} episodes: {}'.format(num_episodes, avg_reward))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Total reward over 500 episodes : [10693.] \nAverage total reward over 500 episodes: 21.38599967956543\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPMvYiSbzVBg",
        "outputId": "4878da7e-a2c9-4ae3-8ed9-7031688c439e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inferences:**  \n",
        "**Step-** Every cycle of state-action-reward is called a step. The reinforcement learning system continues to iterate through cycles until it reaches the desired state or a maximum number of steps are expired.   \n",
        "**Episode-** This series of steps is called an episode. At the beginning of each episode, the environment is set to an initial state and the agentâ€™s reward is reset to zero.  \n",
        "  \n",
        "For our training environment we have taken 500 timesteps and 500 episodes.   \n",
        "\n",
        "* For 500 timesteps the total reward is found to be 488.0 and the average reward is 0.976.  \n",
        "* For 500 episodes the total reward is found to be 10693.0 and the average reward is 21.39.  \n",
        "  \n",
        "The objective of learning is to maximize reward and hence, training over an episode is preferred over timestep."
      ],
      "metadata": {
        "id": "4yhs0c1muDzU"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python",
      "language": "python",
      "display_name": "Pyolite (preview)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernel_info": {
      "name": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}