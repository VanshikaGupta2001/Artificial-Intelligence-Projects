{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#WEEK 8 MDP & DYNAMIC PROGRAMMING"
      ],
      "metadata": {
        "id": "SPjAomibq57m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learn  the  optimal  policy  for  the  frozen  lakeenvironment  using  the  Policy  Iteration  vs  the Value Iteration technique."
      ],
      "metadata": {
        "id": "fStU4A8Oq8R2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Create a Policy Iteration function with the following parameters\n",
        "\n",
        "policy: 2D array of a size n(S) x n(A), each cell represents a probability of taking action a in state s.\n",
        "\n",
        "environment: Initialized OpenAI gym environment object\n",
        "\n",
        "discount_factor: MDP discount factor\n",
        "\n",
        "theta:  A  threshold  of  a  value  function  change.  Once  the  update  to  value function is below this number\n",
        "\n",
        "max_iterations: Maximum number of iterations "
      ],
      "metadata": {
        "id": "oZjeW7krq-b3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "47Z0PZJYnXMB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def policy_iteration(policy, environment, discount_factor, theta, max_iterations):\n",
        "    # Extract the number of states and actions from the policy matrix\n",
        "    num_states, num_actions = policy.shape\n",
        "\n",
        "    # Initialize the value function to zeros for all states\n",
        "    value_function = np.zeros(num_states)\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # Policy Evaluation: Evaluate the current policy by iteratively updating the value function until it converges\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in range(num_states):\n",
        "                v = value_function[s]\n",
        "                new_value = 0\n",
        "                for a in range(num_actions):\n",
        "                    prob = policy[s][a]\n",
        "                    next_states_rewards = []\n",
        "                    for prob_next, next_state, reward, done in environment.P[s][a]:\n",
        "                        next_states_rewards.append(prob_next * (reward + discount_factor * value_function[next_state]))\n",
        "                    new_value += prob * np.sum(next_states_rewards)\n",
        "                value_function[s] = new_value\n",
        "                delta = max(delta, np.abs(v - value_function[s]))\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "        # Policy Improvement: Improve the policy by greedily selecting the best action in each state\n",
        "        policy_stable = True\n",
        "        for s in range(num_states):\n",
        "            old_action = np.argmax(policy[s])\n",
        "            action_values = np.zeros(num_actions)\n",
        "            for a in range(num_actions):\n",
        "                for prob_next, next_state, reward, done in environment.P[s][a]:\n",
        "                    action_values[a] += prob_next * (reward + discount_factor * value_function[next_state])\n",
        "            best_action = np.argmax(action_values)\n",
        "            if old_action != best_action:\n",
        "                policy_stable = False\n",
        "            policy[s] = np.eye(num_actions)[best_action]\n",
        "        \n",
        "        # If the policy has not changed in this iteration, we have found the optimal policy\n",
        "        if policy_stable:\n",
        "            break\n",
        "\n",
        "    return policy, value_function\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the FrozenLake-v1 environment\n",
        "env = gym.make('FrozenLake-v1')\n",
        "\n",
        "# Initialize the policy matrix to a random policy\n",
        "policy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n\n",
        "\n",
        "# Set the discount factor, convergence threshold, and maximum number of iterations\n",
        "discount_factor = 0.99\n",
        "theta = 1e-8\n",
        "max_iterations = 1000\n",
        "\n",
        "# Run the policy iteration algorithm\n",
        "optimal_policy, optimal_value_function = policy_iteration(policy, env, discount_factor, theta, max_iterations)\n",
        "\n",
        "# Print the optimal policy and value function\n",
        "print(\"Optimal Policy:\")\n",
        "print(optimal_policy)\n",
        "\n",
        "print(\"Optimal Value Function:\")\n",
        "print(optimal_value_function)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE2IDPB-nhTn",
        "outputId": "c10e8975-0b1d-40ed-f8f0-c7c739d09dec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "Optimal Value Function:\n",
            "[0.54202581 0.49880303 0.4706955  0.4568515  0.55845085 0.\n",
            " 0.35834799 0.         0.59179866 0.64307976 0.6152075  0.\n",
            " 0.         0.7417204  0.86283741 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Value Iteration function with the following parameters\n",
        "\n",
        "a. environment: Initialized OpenAI gym environment object\n",
        "\n",
        "b.discount_factor: MDP discount factor\n",
        "\n",
        "c.theta:  A  threshold  of  a  value  function  change.  Once  the  update  to  value function is below this number\n",
        "\n",
        "d.max_iterations: Maximum number of iterations "
      ],
      "metadata": {
        "id": "ZDUTdUXzqh1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "def value_iteration(env, discount_factor, theta, max_iterations):\n",
        "    # Extract the number of states and actions from the environment object\n",
        "    num_states = env.observation_space.n\n",
        "    num_actions = env.action_space.n\n",
        "    \n",
        "    # Initialize the value function as a vector of zeros with length equal to the number of states\n",
        "    value_function = np.zeros(num_states)\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        delta = 0\n",
        "        for s in range(num_states):\n",
        "            v = value_function[s]\n",
        "            action_values = np.zeros(num_actions)\n",
        "            for a in range(num_actions):\n",
        "                for prob_next, next_state, reward, done in env.P[s][a]:\n",
        "                    action_values[a] += prob_next * (reward + discount_factor * value_function[next_state])\n",
        "            best_value = np.max(action_values)\n",
        "            value_function[s] = best_value\n",
        "            delta = max(delta, np.abs(v - best_value))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    \n",
        "    # Derive the optimal policy from the optimal value function\n",
        "    policy = np.zeros((num_states, num_actions))\n",
        "    for s in range(num_states):\n",
        "        action_values = np.zeros(num_actions)\n",
        "        for a in range(num_actions):\n",
        "            for prob_next, next_state, reward, done in env.P[s][a]:\n",
        "                action_values[a] += prob_next * (reward + discount_factor * value_function[next_state])\n",
        "        best_action = np.argmax(action_values)\n",
        "        policy[s, best_action] = 1.0\n",
        "\n",
        "    return policy, value_function\n"
      ],
      "metadata": {
        "id": "5AXOHjO1nqn-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the FrozenLake-v1 environment\n",
        "env = gym.make('FrozenLake-v1')\n",
        "\n",
        "# Set the discount factor, threshold, and maximum number of iterations for value iteration\n",
        "discount_factor = 0.99\n",
        "theta = 1e-8\n",
        "max_iterations = 10000\n",
        "\n",
        "# Run the value iteration algorithm to obtain the optimal policy and value function\n",
        "policy, value_function = value_iteration(env, discount_factor, theta, max_iterations)\n",
        "\n",
        "# Print the optimal policy and value function\n",
        "print(\"Optimal Policy:\\n\", policy)\n",
        "print(\"Optimal Value Function:\\n\", value_function)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ1KhfK0qmS_",
        "outputId": "2b185b35-f967-4e70-c9dc-6793307e5b28"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            " [[1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "Optimal Value Function:\n",
            " [0.54202581 0.49880303 0.47069551 0.4568515  0.55845085 0.\n",
            " 0.35834799 0.         0.59179866 0.64307976 0.6152075  0.\n",
            " 0.         0.7417204  0.86283741 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare  the number of  wins, average  return  after  1000  episodes andcomment  on which method performed better."
      ],
      "metadata": {
        "id": "PHTO4nMBrkpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "def run_policy(policy, env, num_episodes):\n",
        "    num_wins = 0\n",
        "    total_return = 0\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_return = 0\n",
        "\n",
        "        while not done:\n",
        "            action = np.random.choice(np.arange(env.action_space.n), p=policy[state])\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode_return += reward\n",
        "            state = next_state\n",
        "\n",
        "        total_return += episode_return\n",
        "        if episode_return > 0:\n",
        "            num_wins += 1\n",
        "\n",
        "    return num_wins, total_return / num_episodes\n",
        "\n",
        "\n",
        "# Initialize the FrozenLake-v1 environment\n",
        "env = gym.make('FrozenLake-v1')\n",
        "\n",
        "# Set the discount factor, threshold, and maximum number of iterations for policy and value iteration\n",
        "discount_factor = 0.99\n",
        "theta = 1e-8\n",
        "max_iterations = 10000\n",
        "\n",
        "# Run policy iteration to obtain the optimal policy and value function\n",
        "policy_pi, _ = policy_iteration(np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n, env, discount_factor, theta, max_iterations)\n",
        "\n",
        "# Run value iteration to obtain the optimal policy and value function\n",
        "_, value_vi = value_iteration(env, discount_factor, theta, max_iterations)\n",
        "policy_vi = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "for s in range(env.observation_space.n):\n",
        "    q_values = np.zeros(env.action_space.n)\n",
        "    for a in range(env.action_space.n):\n",
        "        for prob, next_state, reward, done in env.P[s][a]:\n",
        "            q_values[a] += prob * (reward + discount_factor * value_vi[next_state])\n",
        "    best_a = np.argmax(q_values)\n",
        "    policy_vi[s, best_a] = 1.0\n",
        "\n",
        "# Run the policies for 1000 episodes and compare the results\n",
        "num_episodes = 1000\n",
        "num_wins_pi, avg_return_pi = run_policy(policy_pi, env, num_episodes)\n",
        "num_wins_vi, avg_return_vi = run_policy(policy_vi, env, num_episodes)\n",
        "\n",
        "# Print the results\n",
        "print(\"Policy Iteration Results:\")\n",
        "print(\"Number of wins:\", num_wins_pi)\n",
        "print(\"Average return:\", avg_return_pi)\n",
        "\n",
        "print(\"Value Iteration Results:\")\n",
        "print(\"Number of wins:\", num_wins_vi)\n",
        "print(\"Average return:\", avg_return_vi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6gSaYkEqomN",
        "outputId": "2b8738d9-79ed-415b-917c-48c670a47a54"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Iteration Results:\n",
            "Number of wins: 725\n",
            "Average return: 0.725\n",
            "Value Iteration Results:\n",
            "Number of wins: 736\n",
            "Average return: 0.736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N5H3NYEIrnkf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}