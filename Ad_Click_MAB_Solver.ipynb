{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKKjUng8w4RX",
        "outputId": "81c39b01-797f-4fd5-fbde-b8160578d4f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Ad 1  Ad 2  Ad 3  Ad 4  Ad 5  Ad 6  Ad 7  Ad 8  Ad 9  Ad 10\n",
            "0     1     0     0     0     1     0     0     0     1      0\n",
            "1     0     0     0     0     0     0     0     0     1      0\n",
            "2     0     0     0     0     0     0     0     0     0      0\n",
            "3     0     1     0     0     0     0     0     1     0      0\n",
            "4     0     0     0     0     0     0     0     0     0      0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Use pandas to read the file into a DataFrame\n",
        "data = pd.read_csv('/content/Ads_Optimisation.csv')\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4i9rv9VQWyz"
      },
      "source": [
        "###**QA.  Write down the MAB agent problem formulation in your own words**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY0o4P9YRMx9"
      },
      "source": [
        "The Multi-Armed Bandit (MAB) problem is a framework that involves selecting actions from a set of alternatives (arms) in a repeated manner. \n",
        "\n",
        "Each arm has an unknown reward distribution, and the goal is to maximize the total rewards accumulated over time. In this specific problem, the MAB agent is tasked with selecting the advertisement to display to a user in order to maximize the number of clicks on the ad. The MAB agent chooses an arm, which corresponds to an ad, and observes a reward, which is either 1 (if the user clicks on the ad) or 0 (if the user does not click on the ad). \n",
        "\n",
        "The MAB agent uses the information gained from previous arm selections and rewards to make better decisions in the future, while balancing the exploration of new arms with the exploitation of the best-performing arm so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NetNBOwSLYeN"
      },
      "outputs": [],
      "source": [
        "# Define the MAB agent class\n",
        "class MABAgent:\n",
        "    \n",
        "\n",
        "    '''This code defines a multi-armed bandit (MAB) agent class, which is used to solve a MAB problem. \n",
        "    A MAB problem involves choosing one action out of multiple actions in order to maximize the reward. \n",
        "    The MABAgent class is initialized with the number of actions (n_arms), \n",
        "    the epsilon value for the epsilon-greedy algorithm (epsilon), \n",
        "    and the c value for the UCB algorithm (c). \n",
        "    The class also initializes the Q-value estimates and the number of times an action was selected (N), \n",
        "    both of which are initialized to zero.\n",
        "    '''\n",
        "    def __init__(self, n_arms, epsilon=None, c=None):\n",
        "        self.n_arms = n_arms\n",
        "        self.epsilon = epsilon\n",
        "        self.c = c\n",
        "        self.Q = np.zeros(n_arms)\n",
        "        self.N = np.zeros(n_arms)\n",
        "        self.total_rewards = 0\n",
        "\n",
        "\n",
        "\n",
        "    ''' \n",
        "    The select_action method is used to select an action for the agent to take. \n",
        "    If the epsilon value is not None, then it selects an action with probability epsilon by choosing a random action. \n",
        "    Otherwise, if the c value is not None, then it selects an action using the UCB algorithm. \n",
        "    Otherwise, it selects the action with the highest Q-value estimate.\n",
        "    '''    \n",
        "    def select_action(self):\n",
        "        if self.epsilon is not None and np.random.rand() < self.epsilon:\n",
        "            # Select a random action with probability epsilon\n",
        "            action = np.random.randint(self.n_arms)\n",
        "        elif self.c is not None:\n",
        "            # Select an action using the UCB algorithm\n",
        "            t = np.sum(self.N) + 1\n",
        "            ucb = self.Q + self.c * np.sqrt(np.log(t) / (self.N + 1e-6))\n",
        "            action = np.argmax(ucb)\n",
        "        else:\n",
        "            # Select the action with the highest estimated value\n",
        "            action = np.argmax(self.Q)\n",
        "        return action\n",
        "    \n",
        "\n",
        "    '''\n",
        "    The update method is used to update the Q-value estimate and the number of times an action was selected. \n",
        "    It takes in the action that was taken and the reward for taking that action. \n",
        "    It then increments the number of times the action was selected and updates the Q-value estimate using a formula.\n",
        "    '''\n",
        "    \n",
        "    def update(self, action, reward):\n",
        "        # Update the action value estimates and the number of times the action was taken\n",
        "        self.N[action] += 1\n",
        "        self.Q[action] += (reward - self.Q[action]) / self.N[action]\n",
        "        self.total_rewards += reward\n",
        "        \n",
        "\n",
        "    '''\n",
        "    The run method is used to run the MABAgent on a dataset. It iterates over the dataset 2000 times, \n",
        "    selecting an action using the select_action method, getting the reward for taking that action, \n",
        "    and updating the Q-value estimate and number of times the action was selected using the update method.\n",
        "    '''    \n",
        "    def run(self, data):\n",
        "        for t in range(2000):\n",
        "            action = self.select_action()\n",
        "            reward = data.iloc[t, action]\n",
        "            self.update(action, reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwxR3UwcNcib"
      },
      "source": [
        "###**QB a. Compute the total rewards after 2000-time steps using the ε-greedy action for ε=0.01**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6wrKmufLJ2E",
        "outputId": "087dd475-79bd-4c89-e82e-bd15aef5b460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rewards using epsilon=0.01: 338\n"
          ]
        }
      ],
      "source": [
        "# Compute the total rewards using the epsilon-greedy algorithm with epsilon=0.01\n",
        "agent1 = MABAgent(n_arms=10, epsilon=0.01)\n",
        "agent1.run(data)\n",
        "print(f\"Total rewards using epsilon=0.01: {agent1.total_rewards}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stPnCXDrNnRj"
      },
      "source": [
        "###**QB b. Compute the total rewards after 2000-time steps using the ε-greedy action for ε=0.3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQeXDYBsLME6",
        "outputId": "c2ffdca5-ef42-42ab-da60-feb089eb5fde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rewards using epsilon=0.3: 403\n"
          ]
        }
      ],
      "source": [
        "# Compute the total rewards using the epsilon-greedy algorithm with epsilon=0.3\n",
        "agent2 = MABAgent(n_arms=10, epsilon=0.3)\n",
        "agent2.run(data)\n",
        "print(f\"Total rewards using epsilon=0.3: {agent2.total_rewards}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c4g4p90Nu-0"
      },
      "source": [
        "###**QC. Compute the total rewards after 2000-time steps using the Upper-Confidence-Bound action method for c= 1.5**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbpxnXbwN1kH",
        "outputId": "ad9bc3e8-478a-4baf-e945-fa66c9ebefd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total rewards using UCB with c=1.5: 319\n"
          ]
        }
      ],
      "source": [
        "# Compute the total rewards using the UCB algorithm with c=1.5\n",
        "agent3 = MABAgent(n_arms=10, c=1.5)\n",
        "agent3.run(data)\n",
        "print(f\"Total rewards using UCB with c=1.5: {agent3.total_rewards}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgoWSmFVQD5I"
      },
      "source": [
        "###**QD. For all approaches, explain how the action value estimated compares to the optimal action**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjpnCNQ4T7cq"
      },
      "source": [
        "Based on the output of the code, the total rewards obtained by each approach are as follows:\n",
        "\n",
        "**Epsilon-greedy with epsilon=0.01: 338**\n",
        "\n",
        "**Epsilon-greedy with epsilon=0.3: 403**\n",
        "\n",
        "**UCB with c=1.5: 319**\n",
        "\n",
        "The epsilon-greedy approach with epsilon=0.3 obtained the highest total rewards, followed by the epsilon-greedy approach with epsilon=0.01 and then the UCB approach with c=1.5. This suggests that the epsilon-greedy approach with a higher value of epsilon (0.3) was able to explore more and find better actions than the epsilon-greedy approach with a lower value of epsilon (0.01) and the UCB approach."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}